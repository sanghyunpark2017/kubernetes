
#설치 할 때는 API서버 L4 대신 staic server를 바라보게 하여 진행 권장
#manifast 설치 하기 전에 kubelet 중지하고 진행

##############################
Before you begin
##############################

-Three machines that meet kubeadm’s minimum requirements for the masters
-Three machines that meet kubeadm’s minimum requirements for the workers
-Full network connectivity between all machines in the cluster (public or private network is fine)
-SSH access from one device to all nodes in the system (마스터 3대와 워커 3대)
-sudo privileges on all machines
-그리고 External Etcd 3대

#사전작업은 master 3대, worker 3대, etcd 3대 모두 실행한다.

10.10.64.61 master1
10.10.64.62 master2
10.10.64.63 master3
10.10.64.64 worker1
10.10.64.65 worker2
10.10.64.66 worker3
10.10.64.99 L4


################################
## 사전작업 - 서버설정
################################
vi /etc/fstab

swapoff -a

systemctl disable firewalld
systemctl stop firewalld

#API Server : L4(10.10.64.99), FQDN(kubeapi.homeplusnet.co.kr)


#설치할 때는 MASTER서버 각자 자기 자신을 바라보게함(모든 설치가 끝나면 L4로 조정)
cat << EOF >> /etc/hosts
10.10.64.61  t1vkubeadm1 kubeapi.homeplusnet.co.kr
10.10.64.62  t1vkubeadm2
10.10.64.63  t1vkubeadm3
EOF




#SELINUX 변경(enforcing -> disabled)
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
cat  /etc/selinux/config


shutdown -r now


################################
## 사전작업 - SSH 
################################

ssh-keygen -t rsa

ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeadm1
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeadm2
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeadm3



################################################################
## 사전작업 - 도커설치
################################################################

###Installing Docker
#Docker Version 17.03 is recommended. Versions 17.06+ might work

sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum list docker-ce --showduplicates | sort -r

yum install -y --setopt=obsoletes=0 \
  docker-ce-17.03.1.ce-1.el7.centos \
  docker-ce-selinux-17.03.1.ce-1.el7.centos

systemctl enable docker && systemctl start docker
systemctl status docker

docker --version

#Docker version 17.03.1-ce, build c6d412e

################################################################
## 사전작업 - kubelet kubeadm kubectl 설치
################################################################

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

systemctl status kubelet




#kubelet 수초 간격으로 재시작&오류가 발생하는데, 나중에 kubeadm init 이후에는 정상되는 것으로 지금은 그냥 무시


###cgroup 드라이버를 도커와 kube간에 일치 시켜줘야함
#yum으로 설치하면 systemd, docker ce로 설치하면 cgroup 임. 
#v1.11버전에서는 자동으로 맞춰주므로 PASS
docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

#만약 systemd가 아니고 cgroupfs인 경우
#sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl daemon-reload
systemctl restart kubelet

systemctl status kubelet

#>>>>>>>> master 3대, worker 3대 모두 설치 후 다음 단계 진행


################################################################
## 사전작업 - master 3대 앞에 로드발랜서 구성 (HAPROXY 서버 1대)
################################################################

#kube-apiserver(Listen 6443)로드발랜싱을 위한 tcp 포워딩 로드발란서

#haproxy 설치
#아래 설정이어야 lb가 제대로 동작

cd /etc/haproxy
mv haproxy.cfg haproxy.cfg.bak


vi /etc/haproxy/haproxy.cfg
--------------------------------------------------------------------------------------------------
global
   log /dev/log local0
   log /dev/log local1 notice
   chroot /var/lib/haproxy
   stats socket /run/haproxy/admin.sock mode 660 level admin
   stats timeout 30s
   user haproxy
   group haproxy
   daemon

defaults
   log global
   mode http
   option httplog
   option dontlognull
   timeout connect 5000
   timeout client 50000
   timeout server 50000

frontend k8s-api
  bind *:6443  
  mode tcp
  option tcplog
  default_backend k8s-api

backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server t1vkubeadm1 10.10.64.61:6443 check fall 3 rise 2
  server t1vkubeadm2 10.10.64.62:6443 check fall 3 rise 2
  server t1vkubeadm3 10.10.64.63:6443 check fall 3 rise 2
--------------------------------------------------------------------------------------------------


systemctl enable haproxy.service
systemctl restart haproxy.service
systemctl status haproxy.service

 
firewall-cmd --get-default-zone

firewall-cmd --permanent --zone=public --add-port=6443/tcp

firewall-cmd --reload
firewall-cmd --list-ports

#master에서

yum install -v nc

nc -v kubeapi.homeplusnet.co.kr 6443
>>현재는 refuse 나면 ok


################################################################
################################################################
################################################################
### START(master와 etcd를 같이 실행)
################################################################
################################################################
################################################################

################################################################
## master 1번 서버 설치
################################################################

################################
##  - master1
################################

vi /root/kubeadm-config.yaml
----------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://10.10.64.61:2379"
      advertise-client-urls: "https://10.10.64.61:2379"
      listen-peer-urls: "https://10.10.64.61:2380"
      initial-advertise-peer-urls: "https://10.10.64.61:2380"
      initial-cluster: "t1vkubeadm1=https://10.10.64.61:2380"
    serverCertSANs:
      - t1vkubeadm1
      - 10.10.64.61
    peerCertSANs:
      - t1vkubeadm1
      - 10.10.64.61
networking:
    # This CIDR is a Calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
----------------------------------------------------------------------------------------------

cd ~
kubeadm init --config kubeadm-config.yaml



# 성공적인 init 이후
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#coredns 2개를 제외한 나머지 5개 POD running이면 다음 진행 (coredns는 CNI 설치 후 running됨)
kubectl get nodes
kubectl get pods --namespace=kube-system -o=wide


#init 이후에 master 1번 서버에 생성된 파일들을 다른 master서버로 복사

USER=root # customizable
CONTROL_PLANE_IPS="10.10.64.62 10.10.64.63"
for host in ${CONTROL_PLANE_IPS}; do
    scp /etc/kubernetes/pki/ca.crt "${USER}"@$host:
    scp /etc/kubernetes/pki/ca.key "${USER}"@$host:
    scp /etc/kubernetes/pki/sa.key "${USER}"@$host:
    scp /etc/kubernetes/pki/sa.pub "${USER}"@$host:
    scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host:
    scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host:
    scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key "${USER}"@$host:etcd-ca.key
    scp /etc/kubernetes/admin.conf "${USER}"@$host:
done


################################################################
## master 2번 서버 설치
################################################################

vi /root/kubeadm-config.yaml
----------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://10.10.64.62:2379"
      advertise-client-urls: "https://10.10.64.62:2379"
      listen-peer-urls: "https://10.10.64.62:2380"
      initial-advertise-peer-urls: "https://10.10.64.62:2380"
      initial-cluster: "t1vkubeadm1=https://10.10.64.61:2380,t1vkubeadm2=https://10.10.64.62:2380"
      initial-cluster-state: existing
    serverCertSANs:
      - t1vkubeadm2
      - 10.10.64.62
    peerCertSANs:
      - t1vkubeadm2
      - 10.10.64.62
networking:
    # This CIDR is a calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
----------------------------------------------------------------------------------------------

#master 1번에서 가져온 파일을 제 위치로 이동
USER=root # customizable
mkdir -p /etc/kubernetes/pki/etcd
mv /${USER}/ca.crt /etc/kubernetes/pki/
mv /${USER}/ca.key /etc/kubernetes/pki/
mv /${USER}/sa.pub /etc/kubernetes/pki/
mv /${USER}/sa.key /etc/kubernetes/pki/
mv /${USER}/front-proxy-ca.crt /etc/kubernetes/pki/
mv /${USER}/front-proxy-ca.key /etc/kubernetes/pki/
mv /${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /${USER}/admin.conf /etc/kubernetes/admin.conf

## 중요!!! 이 명령어가 매우 중요함!!!!!!!!!! (도큐에 없음 도뀨!!)
systemctl stop kubelet

kubeadm alpha phase certs all --config kubeadm-config.yaml
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml

systemctl start kubelet
systemctl status kubelet


#master 1번 서버에서 node와 POD 확인
#node는 추가되고 role은 none 상태임, proxy 1개가 추가됨
kubectl get nodes -o=wide
kubectl get pods --namespace=kube-system -o=wide


#노드를 etcd cluster에 추가
#중요!!!! 2번 마스터로 이동
#중요!!!! 2번 마스터로 이동
CP0_IP=10.10.64.61
CP0_HOSTNAME=t1vkubeadm1
CP1_IP=10.10.64.62
CP1_HOSTNAME=t1vkubeadm2

#중요! 2개 명령어를 동시에 실행함.
KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP1_HOSTNAME} https://${CP1_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-config.yaml

#>>>>> etcd cluster add로 인해 master 1번에서 kubectl 수십 초에서 수분 동안 동작하지 않음
#>>>>> master 2번의 etcd pod 1개가 추가됨을 확인


#Deploy the control plane components and mark the node as a master:
kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml
kubeadm alpha phase controlplane all --config kubeadm-config.yaml
kubeadm alpha phase mark-master --config kubeadm-config.yaml

#>>>>> master2번의 api, controller, scheduler pod 등 총 3개가 추가로 running됨




################################################################
## master 3번 서버 설치
################################################################

#master 1/2번 서버의 config와 다름

vi /root/kubeadm-config.yaml
----------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://10.10.64.63:2379"
      advertise-client-urls: "https://10.10.64.63:2379"
      listen-peer-urls: "https://10.10.64.63:2380"
      initial-advertise-peer-urls: "https://10.10.64.63:2380"
      initial-cluster: "t1vkubeadm1=https://10.10.64.61:2380,t1vkubeadm2=https://10.10.64.62:2380,t1vkubeadm3=https://10.10.64.63:2380"
      initial-cluster-state: existing
    serverCertSANs:
      - t1vkubeadm3
      - 10.10.64.63
    peerCertSANs:
      - t1vkubeadm3
      - 10.10.64.63
networking:
    # This CIDR is a calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
----------------------------------------------------------------------------------------------

#master 1번에서 가져온 파일을 제 위치로 이동
USER=root # customizable
mkdir -p /etc/kubernetes/pki/etcd
mv /${USER}/ca.crt /etc/kubernetes/pki/
mv /${USER}/ca.key /etc/kubernetes/pki/
mv /${USER}/sa.pub /etc/kubernetes/pki/
mv /${USER}/sa.key /etc/kubernetes/pki/
mv /${USER}/front-proxy-ca.crt /etc/kubernetes/pki/
mv /${USER}/front-proxy-ca.key /etc/kubernetes/pki/
mv /${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /${USER}/admin.conf /etc/kubernetes/admin.conf


#Run the kubeadm phase commands to bootstrap the kubelet:

systemctl stop kubelet

kubeadm alpha phase certs all --config kubeadm-config.yaml
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml

systemctl start kubelet
systemctl status kubelet

#>>>>> 3번 호스트의 proxy pod가 생성됨

#Run the commands to add the node to the etcd cluster:
CP0_IP=10.10.64.61
CP0_HOSTNAME=t1vkubeadm1
CP2_IP=10.10.64.63
CP2_HOSTNAME=t1vkubeadm3

#동시에 실행함
KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP2_HOSTNAME} https://${CP2_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-config.yaml

#>>>>> 이제 3번 호스트의 etcd pod가 생성됨




#Deploy the control plane components and mark the node as a master:
kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml
kubeadm alpha phase controlplane all --config kubeadm-config.yaml
kubeadm alpha phase mark-master --config kubeadm-config.yaml

#>>>>> 3번 호스트의 api, controller, scheduler 3개가 추가로 running됨

#master 1번 서버에서 master 된 것 확인 (아직 rule은 none 상태)
kubectl get nodes -o=wide  
kubectl get pod --all-namespaces

#core-dns는 pending 상태이나 CNI 설치 후에는 running



mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


#이제 각 마스터의 FQDN을 L4주소로 지정
#만약 자기 자신을 바라보게 놔둘 경우 서버 리붓 후 api server 오류(authorize 날 수 있으니 꼭 변경한다

#master 1번
vi /etc/hosts
10.10.64.61  t1vkubeadm1 kubeapi.homeplusnet.co.kr
10.10.64.62  t1vkubeadm2
10.10.64.63  t1vkubeadm3
10.10.64.99   kubeapi.homeplusnet.co.kr



################################################################
################################################################
################################################################
###       END (master + etcd)
################################################################
################################################################
################################################################


########################
WOKRER NODE 설치
########################

docker, kubelet 등 설치

vi /etc/fstab

cat << EOF >> /etc/hosts
10.10.64.99  kubeapi.homeplusnet.co.kr
EOF

#앞서 master 서버 설치 시 나온 명령어 실행
kubeadm join kubeapi.homeplusnet.co.kr:6443 --token ******* --discovery-token-ca-cert-hash sha256:*******************

#만약, create a new valid toke 에러가 나오면 마스터에서 "kubeadm token create" 후 나온 토큰을
#위 명령어 "--token" 뒤에 넣어서 실행하면됨


################################################################
################################################################
################################################################
Installing a pod network add-on
################################################################
################################################################
################################################################

#master1번 서버에서만 실행
kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
 
kubectl get pods --namespace=kube-system
 
#calico-node-**** POD가 master/worker node 수 만큼 생성됨
#그 후 coredns pod 2개도 running 상태로 변경됨
#만약 coredns가 여전히 error 상태라면 master 서버 롤링 리붓하면 정상화됨 (정상적인 진행이라면 이런 일은 없어야 함)


##########################################
#대시보드 Heapster, influxdb & grafana 설치
##########################################

yum install -y git
mkdir -p  /root/k8s-addon
cd /root/k8s-addon
git clone https://github.com/DragOnMe/kubernetes-mod-skydns-dashboard-mon.git

cd /root/k8s-addon/kubernetes-mod-skydns-dashboard-mon
kubectl create -f Dashboard/dashboard-controller.yaml
kubectl create -f Dashboard/dashboard-service.yaml

#deployment, pod, svc 3개가 생성됨
kubectl get pods -n=kube-system -o=wide
kubectl get deployment -n=kube-system  -o=wide
kubectl get svc -n=kube-system -o=wide

#서비스 속성을 보면 Cluster IP로 되어 있어 외부에서 대시보드 호출 할 수 없음
kubectl describe svc kubernetes-dashboard -n=kube-system
--------------------------------------------------------------------------------------
Name:              kubernetes-dashboard
Namespace:         kube-system
Labels:            k8s-app=kubernetes-dashboard
                   kubernetes.io/cluster-service=true
Annotations:       <none>
Selector:          k8s-app=kubernetes-dashboard
Type:              ClusterIP
IP:                10.102.49.8
Port:              <unset>  80/TCP
TargetPort:        9090/TCP
Endpoints:         192.168.5.3:9090
Session Affinity:  None
Events:            <none>
--------------------------------------------------------------------------------------

#그래서 이 것은 삭제하고
kubectl delete svc kubernetes-dashboard -n=kube-system

#대시보드 deploymemt를 가지고 Node Port의 서비스를 새로 만들고
#Node Port가 default 30000-32767 사이에서 랜덤하게 바인딩 되어
kubectl -n kube-system expose deployment kubernetes-dashboard --name kubernetes-dashboard-nodeport --type=NodePort

#Node Port가 무엇인지 확인해서
kubectl get svc -n=kube-system -o=wide

#브라우저에서 접속하면 되는데, 첫화면에서 서비스계정 에러가 나옴
"replicationcontrollers is forbidden: User "system:serviceaccount:kube-system:default" cannot list replicationcontrollers in the namespace "default""

#서비스계정을 default 네임스페이스에 생성하고 브라우저에 가서 Refresh(F5)하면 정상 표시됨
kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default



















































##external etcd는 설치 실패함(2018년 7월 초)
-etcd listen port가 기본적으로 LOOPBACK address임
-이 것을 설정을 변경하여 real ip로 변경할수는 있음
-그러나 인증서가 loopback address로 발급되어 real ip로는 app통신이 안됨
-원인 및 해결 시도는 하지 않음



################################################################
################################################################
################################################################
### START(master와 etcd를 따로 실행) 
################################################################
################################################################
################################################################

#etcd 클러스터를 먼저 만들고, cert파일을 master 노드로 복사 후 진행하게 됨

################################################################
## Set up a Highly Availabile etcd Cluster With kubeadm
################################################################


#현재 etcd는 각 마스터서버의 static pod로 실행되고 있고 HA 구성은 아님
#etcd 서버 3대를 별도로 둔다. docker, kubelet, kubeadm이 설치되어 있어야 하고 (앞에 가이드 참고)
#상호간에 ssh, scp가 되어야 함

#Setting up the cluster
#static pod etcd 실행을 위한 kubelet config
#현재 "/etc/kubernetes/manifests" 경로는 없음. 뒤에 가서 생성함.


#etcd 3대 모두 실행
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet


#etcd 1번 서버에서 kubeadm 위한 설정 파일 생성
 
export HOST0=10.10.64.96
export HOST1=10.10.64.97
export HOST2=10.10.64.98
 
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1alpha2"
kind: MasterConfiguration
etcd:
    localEtcd:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
EOF
done


#etcd 1번 서버에서 CA 생성
kubeadm alpha phase certs etcd-ca

#아래 2개 파일이 생성됨
ll /etc/kubernetes/pki/etcd/ca.crt
ll /etc/kubernetes/pki/etcd/ca.key

#rm -f  /etc/kubernetes/pki/etcd/*.*


#그럼 이제 etcd 맴버서버들의 certificate을 임시 디렉토리에 생성 

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
--> 여기서 127.0.0.1 ::1 이 지정됨;


kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete


#etcd 2번 호스트에 파일 복사
USER=root
HOST=${HOST1}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
---------------------------------------------
  sudo -Es
    chown -R root:root pki
    mv pki /etc/kubernetes/
    exit
  exit
---------------------------------------------
   
#etcd 3번 호스트에 파일 복사
USER=root
HOST=${HOST2}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
---------------------------------------------
  sudo -Es
    chown -R root:root pki
    mv pki /etc/kubernetes/
    exit
  exit
---------------------------------------------


#etcd 1/2/3 모두 ca확인
yum install -y tree

#etcd 1
clear
tree /tmp/${HOST0}
tree /etc/kubernetes/pki

#etcd 2/3
#etcd 1번 서버와 다른 점은 ca.key 파일이 없어야 함(앞서 삭제함)
clear
tree /root
tree /etc/kubernetes/pki

#Create the static pod manifests
t1vkubeetcd1# kubeadm alpha phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
t1vkubeetcd2# kubeadm alpha phase etcd local --config=/root/kubeadmcfg.yaml
t1vkubeetcd3# kubeadm alpha phase etcd local --config=/root/kubeadmcfg.yaml

#"/etc/kubernetes/manifests/etcd.yaml" 파일이 생성됨
#이 때 리슨포트는 127.0.0.1로 되어있으나 ALL IP(0.0.0.0)로 변경함
cat /etc/kubernetes/manifests/etcd.yaml

sed -i "s/127.0.0.1/0.0.0.0/g" /etc/kubernetes/manifests/etcd.yaml
cat /etc/kubernetes/manifests/etcd.yaml


#maiefests pod 실행
systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet

systemctl restart docker
systemctl status docker

#수십 초 후 올라옴(이미지 다운로드 시간 소요)
docker psㄴ
netstat -nlp



#Check the cluster health
docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \
--cert-file /etc/kubernetes/pki/etcd/peer.crt \
--key-file /etc/kubernetes/pki/etcd/peer.key \
--ca-file /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://0.0.0.0:2379 cluster-health



docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \
--cert-file /etc/kubernetes/pki/etcd/peer.crt \
--key-file /etc/kubernetes/pki/etcd/peer.key \
--ca-file /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://127.0.0.1:2379 cluster-health



 
 ###########################
 이제는 master 서버 3대 설정
 ###########################
 
#먼저 etcd서버에서 생성한 ca를 master 서버에 복사한다.


USER=root 
CONTROL_PLANE_HOSTS="10.10.64.61 10.10.64.62 10.10.64.63"
for host in $CONTROL_PLANE_HOSTS; do
    scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:
    scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${USER}"@$host:
    scp /etc/kubernetes/pki/apiserver-etcd-client.key "${USER}"@$host:
done


#master서버에 폴더 생성해둠
mkdir -p /etc/kubernetes/pki/etcd/

#그리고 복사
USER=root 
CONTROL_PLANE_HOSTS="10.10.64.61 10.10.64.62 10.10.64.63"
for host in $CONTROL_PLANE_HOSTS; do
    scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:/etc/kubernetes/pki/etcd/ca.crt
    scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${USER}"@$host:/etc/kubernetes/pki/apiserver-etcd-client.crt
    scp /etc/kubernetes/pki/apiserver-etcd-client.key "${USER}"@$host:/etc/kubernetes/pki/apiserver-etcd-client.key
done


#MASTET 1번서버 

vi /root/kubeadm-config.yaml
------------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
    external:
        endpoints:
        - https://10.10.64.96:2379
        - https://10.10.64.97:2379
        - https://10.10.64.98:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
networking:
    # This CIDR is a calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
------------------------------------------------------------------------------------------------

kubeadm init --config kubeadm-config.yaml

#MASTET 1번서버 (이전방식)

#v1.11은 external etcd를 3.2.17 버전을 지원함
#v1.10에서는 etcd버전이 3.1.12버전임


cat << EOF >> /etc/hosts
10.10.64.60  kubeapi.homeplusnet.co.kr
EOF

ping kubeapi.homeplusnet.co.kr

vi /root/kubeadm-config.yaml
------------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
    external:
        endpoints:
        - https://10.10.64.96:2379
        - https://10.10.64.97:2379
        - https://10.10.64.98:2379
        caFile: /etc/kubernetes/pki/etcd/ca.pem
        certFile: /etc/kubernetes/pki/etcd/client.pem
        keyFile: /etc/kubernetes/pki/etcd/client-key.pem
  
networking:
    # This CIDR is a calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
------------------------------------------------------------------------------------------------

kubeadm init --config kubeadm-config.yaml









------------------------------------------------------------------------------------------------
