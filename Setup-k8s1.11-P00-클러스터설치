

##############################
Before you begin
##############################

-Three machines that meet kubeadm’s minimum requirements for the masters
-Three machines that meet kubeadm’s minimum requirements for the workers
-Full network connectivity between all machines in the cluster (public or private network is fine)
-SSH access from one device to all nodes in the system (마스터 3대와 워커 3대)
-sudo privileges on all machines
-그리고 External Etcd 3대

################################
## 사전작업 - SSH
################################

cat << EOF >> /etc/hosts
#kubernetes master
10.10.64.61  t1vkubeadm1
10.10.64.62  t1vkubeadm2
10.10.64.63  t1vkubeadm3

#kubernetes worker
10.10.64.64  t1vkubework1
10.10.64.65  t1vkubework2
10.10.64.66  t1vkubework3

#kubernetes etcd
10.10.64.96  t1vkubeetcd1
10.10.64.97  t1vkubeetcd2
10.10.64.98  t1vkubeetcd3
EOF

ssh-keygen -t rsa

ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeadm1
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeadm2
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeadm3

ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubework1
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubework2
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubework3

ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeetcd1
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeetcd2
ssh-copy-id -i ~/.ssh/id_rsa.pub root@t1vkubeetcd3


################################################################
## 사전작업 - 도커설치
################################################################

#방화벽 일단 셧다운
systemctl disable firewalld
systemctl stop firewalld

###Installing Docker
#Docker Version 17.03 is recommended. Versions 17.06+ might work

sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum list docker-ce --showduplicates | sort -r

yum install -y --setopt=obsoletes=0 \
  docker-ce-17.03.1.ce-1.el7.centos \
  docker-ce-selinux-17.03.1.ce-1.el7.centos

systemctl enable docker && systemctl start docker
systemctl status docker

docker --version



#Docker version 17.03.1-ce, build c6d412e



################################################################
## 사전작업 - kubelet kubeadm kubectl 설치
################################################################

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl

systemctl enable kubelet && systemctl start kubelet

#SELINUX 변경(enforcing -> disabled)
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
cat  /etc/selinux/config

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


systemctl status kubelet


#kubelet 수초 간격으로 재시작&오류가 발생하는데, 나중에 kubeadm init 이후에는 정상되는 것으로 무시해도됨

###cgroup 드라이버를 도커와 kube간에 일치 시켜줘야함
#yum으로 설치하면 systemd, docker ce로 설치하면 cgroup 
#v1.11버전에서는 뒤에서 자동으로 맞춰줌
docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

#만약 systemd가 아니고 cgroupfs인 경우
#sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl daemon-reload
systemctl restart kubelet

systemctl status kubelet


 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> master 3대, worker 3대 모두 설치 후 다음 단계 진행


################################################################
## 사전작업 - master 3대 앞에 로드발랜서 구성 (HAPROXY 서버 1대)
################################################################

#kube-apiserver(Listen 6443)로드발랜싱을 위한 tcp 포워딩 로드발란서

#haproxy 설치

cd /etc/haproxy
mv haproxy.cfg haproxy.cfg.bak


vi /etc/haproxy/haproxy.cfg
-------------------------------------------------
global
   log /dev/log local0
   log /dev/log local1 notice
   chroot /var/lib/haproxy
   stats socket /run/haproxy/admin.sock mode 660 level admin
   stats timeout 30s
   user haproxy
   group haproxy
   daemon

defaults
   log global
   mode http
   option httplog
   option dontlognull
   timeout connect 5000
   timeout client 50000
   timeout server 50000

frontend k8s-api
  bind 10.10.64.99:6443
  bind 127.0.0.1:6443
  mode tcp
  option tcplog
  default_backend k8s-api

backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server t1vkubeadm1 10.10.64.61:6443 check fall 3 rise 2
  server t1vkubeadm2 10.10.64.62:6443 check fall 3 rise 2
  server t1vkubeadm3 10.10.64.63:6443 check fall 3 rise 2
-------------------------------------------------

systemctl enable haproxy.service
systemctl restart haproxy.service
systemctl status haproxy.service

 
firewall-cmd --get-default-zone

firewall-cmd --permanent --zone=public --add-port=6443/tcp

firewall-cmd --reload
firewall-cmd --list-ports



################################################################
## master 1번 서버 설치
################################################################

#HAPROXY VIP FQDN 등록(나중에 DNS)
cat << EOF >> /etc/hosts
10.10.64.99  kubeapi.homeplusnet.co.kr
EOF

vi /root/kubeadm-config.yaml
----------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://10.10.64.61:2379"
      advertise-client-urls: "https://10.10.64.61:2379"
      listen-peer-urls: "https://10.10.64.61:2380"
      initial-advertise-peer-urls: "https://10.10.64.61:2380"
      initial-cluster: "t1vkubeadm1=https://10.10.64.61:2380"
    serverCertSANs:
      - t1vkubeadm1
      - 10.10.64.61
    peerCertSANs:
      - t1vkubeadm1
      - 10.10.64.61
networking:
    # This CIDR is a Calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
----------------------------------------------------------------------------------------------

systemctl stop firewalld 
systemctl disable firewalld

kubeadm init --config kubeadm-config.yaml

# 성공적인 init 이후

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes
kubectl get pods --namespace=kube-system -o=wide

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 모든 POD가 Running 상태인 것 확인 후 다음 진행(coredns제외)


#master 1번 서버에서 init 이후에 생성된 파일들을 다른 master서버로 복사


vi /root/mycopy.sh
----------------------------------------------------------------------------------------------
USER=root # customizable
CONTROL_PLANE_IPS="10.10.64.62 10.10.64.63"
for host in ${CONTROL_PLANE_IPS}; do
    scp /etc/kubernetes/pki/ca.crt "${USER}"@$host:
    scp /etc/kubernetes/pki/ca.key "${USER}"@$host:
    scp /etc/kubernetes/pki/sa.key "${USER}"@$host:
    scp /etc/kubernetes/pki/sa.pub "${USER}"@$host:
    scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host:
    scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host:
    scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key "${USER}"@$host:etcd-ca.key
    scp /etc/kubernetes/admin.conf "${USER}"@$host:
done
----------------------------------------------------------------------------------------------

chmod 755 mycopy.sh
./mycopy.sh


################################################################
## master 2번 서버 설치
################################################################

#HAPROXY VIP FQDN 등록(나중에 DNS)
cat << EOF >> /etc/hosts
10.10.64.99  kubeapi.homeplusnet.co.kr
EOF


#master 1번 서버의 config와 다름

vi /root/kubeadm-config.yaml
----------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://10.10.64.62:2379"
      advertise-client-urls: "https://10.10.64.62:2379"
      listen-peer-urls: "https://10.10.64.62:2380"
      initial-advertise-peer-urls: "https://10.10.64.62:2380"
      initial-cluster: "t1vkubeadm1=https://10.10.64.61:2380,t1vkubeadm2=https://10.10.64.62:2380"
      initial-cluster-state: existing
    serverCertSANs:
      - t1vkubeadm2
      - 10.10.64.62
    peerCertSANs:
      - t1vkubeadm2
      - 10.10.64.62
networking:
    # This CIDR is a calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
----------------------------------------------------------------------------------------------


#master 1번에서 가져온 파일을 제 위치로 이동

mkdir -p /etc/kubernetes/pki/etcd

mv /root/ca.crt /etc/kubernetes/pki/
mv /root/ca.key /etc/kubernetes/pki/
mv /root/sa.pub /etc/kubernetes/pki/
mv /root/sa.key /etc/kubernetes/pki/
mv /root/front-proxy-ca.crt /etc/kubernetes/pki/
mv /root/front-proxy-ca.key /etc/kubernetes/pki/
mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /root/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /root/admin.conf /etc/kubernetes/admin.conf


kubeadm alpha phase certs all --config kubeadm-config.yaml
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml
systemctl start kubelet

#노드를 etcd cluster에 추가
CP0_IP=10.10.64.61
CP0_HOSTNAME=t1vkubeadm1
CP1_IP=10.10.64.62
CP1_HOSTNAME=t1vkubeadm2

KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP1_HOSTNAME} https://${CP1_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-config.yaml

#Deploy the control plane components and mark the node as a master:

kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml
kubeadm alpha phase controlplane all --config kubeadm-config.yaml
kubeadm alpha phase mark-master --config kubeadm-config.yaml
  
#master 1번에서 master 역활로 변경된 것 확인
kubectl get nodes -o=wide

################################################################
## master 3번 서버 설치
################################################################

#HAPROXY VIP FQDN 등록(나중에 DNS)
cat << EOF >> /etc/hosts
10.10.64.99  kubeapi.homeplusnet.co.kr
EOF


#master 1/2번 서버의 config와 다름

vi /root/kubeadm-config.yaml
----------------------------------------------------------------------------------------------
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- "kubeapi.homeplusnet.co.kr"
api:
    controlPlaneEndpoint: "kubeapi.homeplusnet.co.kr:6443"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://10.10.64.63:2379"
      advertise-client-urls: "https://10.10.64.63:2379"
      listen-peer-urls: "https://10.10.64.63:2380"
      initial-advertise-peer-urls: "https://10.10.64.63:2380"
      initial-cluster: "t1vkubeadm1=https://10.10.64.61:2380,t1vkubeadm2=https://10.10.64.62:2380,t1vkubeadm3=https://10.10.64.63:2380"
      initial-cluster-state: existing
    serverCertSANs:
      - t1vkubeadm3
      - 10.10.64.63
    peerCertSANs:
      - t1vkubeadm3
      - 10.10.64.63
networking:
    # This CIDR is a calico default. Substitute or remove for your CNI provider.
    podSubnet: "192.168.0.0/16"
----------------------------------------------------------------------------------------------

#master 1번에서 가져온 파일을 제 위치로 이동

mkdir -p /etc/kubernetes/pki/etcd

mv /root/ca.crt /etc/kubernetes/pki/
mv /root/ca.key /etc/kubernetes/pki/
mv /root/sa.pub /etc/kubernetes/pki/
mv /root/sa.key /etc/kubernetes/pki/
mv /root/front-proxy-ca.crt /etc/kubernetes/pki/
mv /root/front-proxy-ca.key /etc/kubernetes/pki/
mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /root/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /root/admin.conf /etc/kubernetes/admin.conf

#Run the kubeadm phase commands to bootstrap the kubelet:

kubeadm alpha phase certs all --config kubeadm-config.yaml
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml

systemctl start kubelet
systemctl status kubelet

#Run the commands to add the node to the etcd cluster:

CP0_IP=10.10.64.61
CP0_HOSTNAME=t1vkubeadm1
CP2_IP=10.10.64.63
CP2_HOSTNAME=t1vkubeadm3

KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP2_HOSTNAME} https://${CP2_IP}:2380

kubeadm alpha phase etcd local --config kubeadm-config.yaml

#master 1번 서버에서 join 된 것 확인 (아직 rule은 none 상태)
kubectl get nodes -o=wide

#Deploy the control plane components and mark the node as a master:

kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml
kubeadm alpha phase controlplane all --config kubeadm-config.yaml
kubeadm alpha phase mark-master --config kubeadm-config.yaml

#master 1번 서버에서 master 된 것 확인 (아직 rule은 none 상태)
kubectl get nodes -o=wide  
kubectl get pod --all-namespaces


################################################################
## Set up a Highly Availabile etcd Cluster With kubeadm
################################################################

#현재 etcd는 각 마스터서버의 static pod로 실행되고 있고 HA 구성은 아님
#etcd 서버 3대를 별도로 둔다. docker, kubelet, kubeadm이 설치되어 있어야 하고 (앞에 가이드 참고)
#상호간에 ssh, scp가 되어야 함

#Setting up the cluster

#etcd를 위한 kubelet config

cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet

#etcd 1번 서버에서 kubeadm 위한 설정 파일 생성
 
export HOST0=10.10.64.96
export HOST1=10.10.64.97
export HOST2=10.10.64.98
 
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/


ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1alpha2"
kind: MasterConfiguration
etcd:
    localEtcd:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
EOF
done


#etcd 1번 서버에서 CA 생성
kubeadm alpha phase certs etcd-ca

#아래 2개 파일이 생성됨
ll /etc/kubernetes/pki/etcd/ca.crt
ll /etc/kubernetes/pki/etcd/ca.key

#그럼 이제 etcd 맴버서버의 certificate 생성

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete


#etcd 2번 호스트에 파일 복사
USER=root
HOST=${HOST1}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
   chown -R root:root pki
   mv pki /etc/kubernetes/
   exit
   
#etcd 3번 호스트에 파일 복사
USER=root
HOST=${HOST2}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
   chown -R root:root pki
   mv pki /etc/kubernetes/
   exit

#etcd 1/2/3 모두 ca확인
yum install -y tree

#etcd 1
clear
tree /tmp/${HOST0}
tree /etc/kubernetes/pki

#etcd 2/3
clear
tree /root
tree /etc/kubernetes/pki


#각각의 etcd서버에서 실행
etcd1# kubeadm alpha phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
etcd2# kubeadm alpha phase etcd local --config=/root/kubeadmcfg.yaml
etcd3# kubeadm alpha phase etcd local --config=/root/kubeadmcfg.yaml

#Check the cluster health

docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \
--cert-file /etc/kubernetes/pki/etcd/peer.crt \
--key-file /etc/kubernetes/pki/etcd/peer.key \
--ca-file /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://${HOST0}:2379 cluster-health

@@@@ 혹시 지금 etcd 설치한 것이 etcd 클러스터 맴버 설치 즉, master1/2/3에 했어야 하는건가???



cat /etc/kubernetes/pki/etcd/ca.crt
cat /etc/kubernetes/pki/apiserver-etcd-client.crt
cat /etc/kubernetes/pki/apiserver-etcd-client.key
























