
#############
NOTE!!
#############

1.마스터서버 리붓 또는 복원이후 재시도 시 setenforce 0 
2.DNS POD 서비시 기동위한 yum install -y ebtables ethtool


###########################################
기본
###########################################

###서버스펙
Centos7, 2코어, 2GB메모리 3대

###OS BASIC
cat <<EOF >> /root/.bashrc
export LS_COLORS="di=00;36:fi=00;37"
EOF

vi /etc/fstab

cd /sbin
vi bk
------------------------------------------------------------
#CP.ALL.ENABLE

for i in $*
do
        if [ -f $i ]
        then
                cp -pf "$1" "$1".`date +"%y%m%d.%H%M%S"`
        else
                echo "`basename $0` <filenames>"
        fi
done
------------------------------------------------------------

yum install -y net-tools

######################################################################
#########################################################
Installing prerequisites on masters and workers
#########################################################
#########################################################

###FIREWALL
#포트 상세 정보 : https://kubernetes.io/docs/tasks/tools/install-kubeadm/#before-you-begin

systemctl start firewalld
systemctl enable firewalld
firewall-cmd --get-default-zone

firewall-cmd --permanent --zone=public --add-port=6443/tcp
firewall-cmd --permanent --zone=public --add-port=2379-2380/tcp
firewall-cmd --permanent --zone=public --add-port=10250/tcp
firewall-cmd --permanent --zone=public --add-port=10251/tcp
firewall-cmd --permanent --zone=public --add-port=10252/tcp
firewall-cmd --permanent --zone=public --add-port=10255/tcp
firewall-cmd --reload

firewall-cmd --list-ports

#방화벽 일단 셧다운
systemctl disable firewalld
systemctl stop firewalld

###Installing Docker
yum install -y docker

systemctl enable docker && systemctl start docker
systemctl status docker

###Installing kubeadm, kubelet and kubect
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

vi /etc/selinux/config
SELINUX=disabled


yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet
systemctl status kubelet


cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


###Configure cgroup driver used by kubelet on Master Node
docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

#만약 systemd가 아니고 cgroupfs인 경우
#sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl daemon-reload
systemctl restart kubelet


############################################
###########################################
Setting up an HA etcd cluster
###########################################
############################################

##etcd서버 3대  

systemctl disable firewalld
systemctl stop firewalld

vi /etc/fstab


cat <<EOF >> /etc/hosts
10.10.64.96  t1vkubeetcd1
10.10.64.97  t1vkubeetcd2
10.10.64.98  t1vkubeetcd3
EOF

yum install -y net-tools curl 
curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x /usr/local/bin/cfssl*

mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd

cat >ca-config.json <<EOF
{
   "signing": {
       "default": {
           "expiry": "43800h"
       },
       "profiles": {
           "server": {
               "expiry": "43800h",
               "usages": [
                   "signing",
                   "key encipherment",
                   "server auth",
                   "client auth"
               ]
           },
           "client": {
               "expiry": "43800h",
               "usages": [
                   "signing",
                   "key encipherment",
                   "client auth"
               ]
           },
           "peer": {
               "expiry": "43800h",
               "usages": [
                   "signing",
                   "key encipherment",
                   "server auth",
                   "client auth"
               ]
           }
       }
   }
}
EOF

cat >ca-csr.json <<EOF
{
    "hosts": [
        "t1vkubeetcd1",
        "www.t1vkubeetcd1.com"
    ],
    "CN": "www.t1vkubeetcd1.com",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [{
        "C": "KR",
        "L": "Gangseo-gu",
        "O": "Homeplus, co LTD",
        "OU": "Operations",
        "ST": "Seoul"
    }]
}
EOF


#generate the CA certs:

cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

cat >client.json <<EOF
{
  "CN": "client",
  "key": {
      "algo": "ecdsa",
      "size": 256
  }
}
EOF


#Generate etcd client certs

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client


#etcd 3대 모두
export PEER_NAME=$(hostname)
export PRIVATE_IP=$(ip addr show eth0 | grep -Po 'inet \K[\d.]+')

ssh-keygen -t rsa -b 4096 -C "sanghyun.park@homeplus.co.kr"

#2번과 3번서버에 public key 화면표시
cat ~/.ssh/id_rsa.pub

#1번 서버 authorized_keys에 2번서버,3번서버의 public key 넣어줌
vi ~/.ssh/authorized_keys

#2번과 3번서버에 1번서버의 root CA를 넣어줌
mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/ca.pem .
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/ca-key.pem .
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/client.pem .
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/client-key.pem .
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/ca-config.json .

#3대 모두에서 실행
cfssl print-defaults csr > config.json
sed -i '0,/CN/{s/example\.net/'"$PEER_NAME"'/}' config.json
sed -i 's/www\.example\.net/'"$PRIVATE_IP"'/' config.json
sed -i 's/example\.net/'"$PEER_NAME"'/' config.json
cat config.json


cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server config.json | cfssljson -bare server
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer config.json | cfssljson -bare peer

ETCD_VERSION="v3.1.12" 
curl -sSL https://github.com/coreos/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/local/bin/


touch /etc/etcd.env
echo "PEER_NAME=${PEER_NAME}" >> /etc/etcd.env
echo "PRIVATE_IP=${PRIVATE_IP}" >> /etc/etcd.env



#서버 3대 모두 서비스 기동
#3대 중 2대는 기동되어야 running

======================= 1번서버 etcd 기동

cat >/etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/local/bin/etcd --name t1vkubeetcd1 --data-dir /var/lib/etcd --listen-client-urls https://t1vkubeetcd1:2379 --advertise-client-urls https://t1vkubeetcd1:2379 --listen-peer-urls https://t1vkubeetcd1:2380 --initial-advertise-peer-urls https://t1vkubeetcd1:2380 --cert-file=/etc/kubernetes/pki/etcd/server.pem --key-file=/etc/kubernetes/pki/etcd/server-key.pem --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --initial-cluster t1vkubeetcd1=https://10.10.64.96:2380,t1vkubeetcd2=https://10.10.64.97:2380,t1vkubeetcd3=https://10.10.64.98:2380 --initial-cluster-token my-etcd-token --initial-cluster-state new

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl stop etcd
systemctl enable etcd
systemctl start etcd

systemctl status etcd


======================= 2번서버 etcd 기동


cat >/etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/local/bin/etcd --name t1vkubeetcd2 --data-dir /var/lib/etcd --listen-client-urls https://t1vkubeetcd2:2379 --advertise-client-urls https://t1vkubeetcd2:2379 --listen-peer-urls https://t1vkubeetcd2:2380 --initial-advertise-peer-urls https://t1vkubeetcd2:2380 --cert-file=/etc/kubernetes/pki/etcd/server.pem --key-file=/etc/kubernetes/pki/etcd/server-key.pem --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --initial-cluster t1vkubeetcd1=https://10.10.64.96:2380,t1vkubeetcd2=https://10.10.64.97:2380,t1vkubeetcd3=https://10.10.64.98:2380 --initial-cluster-token my-etcd-token --initial-cluster-state new

[Install]
WantedBy=multi-user.target
EOF


systemctl daemon-reload
systemctl stop etcd
systemctl enable etcd
systemctl start etcd

systemctl status etcd

================================= 3번서버 etcd 기동

cat >/etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/local/bin/etcd --name t1vkubeetcd3 --data-dir /var/lib/etcd --listen-client-urls https://t1vkubeetcd3:2379 --advertise-client-urls https://t1vkubeetcd3:2379 --listen-peer-urls https://t1vkubeetcd3:2380 --initial-advertise-peer-urls https://t1vkubeetcd3:2380 --cert-file=/etc/kubernetes/pki/etcd/server.pem --key-file=/etc/kubernetes/pki/etcd/server-key.pem --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --initial-cluster t1vkubeetcd1=https://10.10.64.96:2380,t1vkubeetcd2=https://10.10.64.97:2380,t1vkubeetcd3=https://10.10.64.98:2380 --initial-cluster-token my-etcd-token --initial-cluster-state new

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl stop etcd
systemctl enable etcd
systemctl start etcd

systemctl status etcd

####################################################################################
####################################################################################
master서버 로드밸런싱 설정 
####################################################################################
####################################################################################

#MASTER 3대 모두

yum install -y keepalived

cd /etc/keepalived
bk keepalived.conf 


#로드밸런싱설정(master1)
cat << EOF > /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
 router_id LVS_DEVEL
}

vrrp_script check_apiserver {
 script "/etc/keepalived/check_apiserver.sh"
 interval 3
 weight -2
 fall 10
 rise 2
}

vrrp_instance VI_1 {
   state MASTER  
   interface eth0
   virtual_router_id 51
   priority 101   
   authentication {
       auth_type PASS
       auth_pass 4be37dc3b4c90194d1600c483e10ad1d
   }
   virtual_ipaddress {
       10.10.64.60
   }
   track_script {
       check_apiserver
   }
}
EOF

#로드밸런싱설정(master2)
cat << EOF > /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
 router_id LVS_DEVEL
}

vrrp_script check_apiserver {
 script "/etc/keepalived/check_apiserver.sh"
 interval 3
 weight -2
 fall 10
 rise 2
}

vrrp_instance VI_1 {
   state BACKUP  
   interface eth0
   virtual_router_id 51
   priority 100   
   authentication {
       auth_type PASS
       auth_pass 4be37dc3b4c90194d1600c483e10ad1d
   }
   virtual_ipaddress {
       10.10.64.60
   }
   track_script {
       check_apiserver
   }
}
EOF


#로드밸런싱설정(master3)
cat << EOF > /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
 router_id LVS_DEVEL
}

vrrp_script check_apiserver {
 script "/etc/keepalived/check_apiserver.sh"
 interval 3
 weight -2
 fall 10
 rise 2
}

vrrp_instance VI_1 {
   state BACKUP  
   interface eth0
   virtual_router_id 51
   priority 99   
   authentication {
       auth_type PASS
       auth_pass 4be37dc3b4c90194d1600c483e10ad1d
   }
   virtual_ipaddress {
       10.10.64.60
   }
   track_script {
       check_apiserver
   }
}
EOF



#공통
vi /etc/keepalived/check_apiserver.sh
--------------------------------------------------
#!/bin/sh

errorExit() {
   echo "*** $*" 1>&2
   exit 1
}

curl --silent --max-time 2 --insecure https://localhost:6443/ -o /dev/null || errorExit "Error GET https://localhost:6443/"
if ip addr | grep -q 10.10.64.60; then
   curl --silent --max-time 2 --insecure https://10.10.64.60:6443/ -o /dev/null || errorExit "Error GET https://10.10.64.60:6443/"
fi
--------------------------------------------------
chmod 755  /etc/keepalived/check_apiserver.sh



systemctl enable keepalived
systemctl stop keepalived
systemctl start keepalived
systemctl status keepalived


##########################################
##########################################
haproxy  => 실패,,,,,
##########################################
##########################################

yum install -y net-tools

cat <<EOF >> /etc/hosts
10.10.64.61  t1vkubeadm1
10.10.64.62  t1vkubeadm2
10.10.64.63  t1vkubeadm3
EOF


yum info haproxy

yum install wget gcc pcre-static pcre-devel -y
 
wget http://www.haproxy.org/download/1.6/src/haproxy-1.6.3.tar.gz -O ~/haproxy.tar.gz

tar xzvf ~/haproxy.tar.gz -C ~/

cd ~/haproxy-1.6.3

make TARGET=linux2628

sudo make install

sudo cp /usr/local/sbin/haproxy /usr/sbin/
sudo cp ~/haproxy-1.6.3/examples/haproxy.init /etc/init.d/haproxy
sudo chmod 755 /etc/init.d/haproxy


sudo mkdir -p /etc/haproxy
sudo mkdir -p /run/haproxy
sudo mkdir -p /var/lib/haproxy
sudo touch /var/lib/haproxy/stats

sudo useradd -r haproxy

sudo haproxy –v

sudo firewall-cmd --permanent --zone=public --add-service=http
sudo firewall-cmd --permanent --zone=public --add-port=30000-32727/tcp
sudo firewall-cmd --reload



vi /etc/haproxy/haproxy.cfg
-------------------------------------------------
global
   log /dev/log local0
   log /dev/log local1 notice
   chroot /var/lib/haproxy
   stats socket /run/haproxy/admin.sock mode 660 level admin
   stats timeout 30s
   user haproxy
   group haproxy
   daemon

defaults
   log global
   mode http
   option httplog
   option dontlognull
   timeout connect 5000
   timeout client 50000
   timeout server 50000

frontend k8s-api
  bind 10.10.64.60:6443
  bind 127.0.0.1:6443
  mode tcp
  option tcplog
  default_backend k8s-api

backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server t1vkubeadm1 10.10.64.61:6443 check fall 3 rise 2
  server t1vkubeadm2 10.10.64.62:6443 check fall 3 rise 2
  server t1vkubeadm3 10.10.64.63:6443 check fall 3 rise 2
-------------------------------------------------

systemctl enable haproxy.service
systemctl restart haproxy.service
systemctl status haproxy.service


##########################################
##########################################
Run kubeadm init on master0
##########################################
##########################################

#MASTER서버 3대 모두

yum install -y ebtables ethtool


systemctl stop firewalld
systemctl disable firewalld

cat <<EOF >> /etc/hosts
10.10.64.61  t1vkubeadm1
10.10.64.62  t1vkubeadm2
10.10.64.63  t1vkubeadm3
EOF

export PEER_NAME=$(hostname)
export PRIVATE_IP=$(ip addr show eth0 | grep -Po 'inet \K[\d.]+')

echo $PEER_NAME
echo $PRIVATE_IP

ssh-keygen -t rsa -b 4096 -C "sanghyun.park@homeplus.co.kr"

cat ~/.ssh/id_rsa.pub

#etcd0서버의 authorized_keys에 master서버 3대의 puliic 을 넣어준다
cd ~/.ssh
bk authorized_keys
vi ~/.ssh/authorized_keys


#MASTER서버 3대 모두
mkdir -p /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/ca.pem /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/client.pem /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/client-key.pem /etc/kubernetes/pki/etcd
ll /etc/kubernetes/pki/etcd
 

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
#중요 복원 후 재시도할 떄 setenforce 0 이후 할 것
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#MASTER 1번 서버 init
#1번 마스터 올라오면,,,,,,, CNI설치한 후에 나머지 마스터 init할 것

cd ~
cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 10.10.64.60
  controlPlaneEndpoint: 10.10.64.60
etcd:
  endpoints:
  - https://10.10.64.96:2379
  - https://10.10.64.97:2379
  - https://10.10.64.98:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: 192.168.0.0/16
apiServerCertSANs:
- 10.10.64.60
- 10.10.64.61
apiServerExtraArgs:
  apiserver-count: "3"
EOF

kubeadm init --config=config.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes
kubectl get pods --namespace=kube-system

#DNS 컨테이너를 제외한 나머지가 RUNNING이면 CNI 설치함 (CNI 설치하기 전에는 PENDING 상태임)

##################################
Install CNI network
##################################
#master1번 서버에서만 실행

kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

kubectl get pods --namespace=kube-system

#모두 running 된 것 확인 후 나머지 master와 worker 진행


##########################################
Run kubeadm init on master1 and master2
##########################################

#master2 & 3 public
cat ~/.ssh/id_rsa.pub

#master1번 서버에 master2와 master3 public 넣어준다
vi ~/.ssh/authorized_keys

#master2와 3번 서버에 1번서버의 pki들을 가져간다
scp root@10.10.64.61:/etc/kubernetes/pki/* /etc/kubernetes/pki
rm -f /etc/kubernetes/pki/apiserver*


#MASTER 2번 서버 init

cd ~
cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 10.10.64.60
  controlPlaneEndpoint: 10.10.64.60
etcd:
  endpoints:
  - https://10.10.64.96:2379
  - https://10.10.64.97:2379
  - https://10.10.64.98:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: 192.168.0.0/16
apiServerCertSANs:
- 10.10.64.60
- 10.10.64.62
apiServerExtraArgs:
  apiserver-count: "3"
EOF

kubeadm init --config=config.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get pods --namespace=kube-system




#MASTER 3번 서버

cd ~
cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 10.10.64.60
  controlPlaneEndpoint: 10.10.64.60
etcd:
  endpoints:
  - https://10.10.64.96:2379
  - https://10.10.64.97:2379
  - https://10.10.64.98:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: 192.168.0.0/16
apiServerCertSANs:
- 10.10.64.60
- 10.10.64.63
apiServerExtraArgs:
  apiserver-count: "3"
EOF


kubeadm init --config=config.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config




#### #### ######## #### ######## #### ######## #### ####

kube클러스터 리셋 시

systemctl stop kubelet
systemctl stop docker
systemctl start docker
systemctl start kubelet

#master2와 3번 서버에 1번서버의 pki들을 가져간다
scp root@10.10.64.61:/etc/kubernetes/pki/* /etc/kubernetes/pki
rm -f /etc/kubernetes/pki/apiserver*



mkdir -p /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/ca.pem /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/client.pem /etc/kubernetes/pki/etcd
scp root@10.10.64.96:/etc/kubernetes/pki/etcd/client-key.pem /etc/kubernetes/pki/etcd
ll /etc/kubernetes/pki/etcd

#### #### ######## #### ####




##################################
##################################
Install workers
##################################
##################################

MASTER서버와 동일하게, 도커설치하고 selinux diabled, kubelet, kubeadm, kube 등 설치한다 



cat <<EOF >> /etc/hosts
10.10.64.61  t1vkubemaster1
10.10.64.62  t1vkubemaster2
10.10.64.63  t1vkubemaster3

10.10.64.64  t1vkubework1
10.10.64.65  t1vkubework2
10.10.64.66  t1vkubework3
EOF

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
sysctl net.bridge.bridge-nf-call-iptables=1

kubeadm join ~~~~~~~~~~~~~~~~~~~~~~~

#모두 running 확인 후 진행
kubectl get pods --namespace=kube-system
kubectl get nodes


















