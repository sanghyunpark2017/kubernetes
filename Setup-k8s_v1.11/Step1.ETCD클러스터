

################################
## 서버구성
################################

10.10.64.96 t1vkubeetcd1
10.10.64.97 t1vkubeetcd2
10.10.64.98 t1vkubeetcd3


################################
## 사전작업 - 서버설정
################################
vi /etc/fstab

swapoff -a

systemctl disable firewalld
systemctl stop firewalld

#SELINUX 변경(enforcing -> disabled)
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
cat  /etc/selinux/config

shutdown -r now



################################################################
## 사전작업 - 도커설치
################################################################

###Installing Docker
#Docker Version 17.03 is recommended. Versions 17.06+ might work

sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum list docker-ce --showduplicates | sort -r

yum install -y --setopt=obsoletes=0 \
  docker-ce-17.03.1.ce-1.el7.centos \
  docker-ce-selinux-17.03.1.ce-1.el7.centos

systemctl enable docker && systemctl start docker
systemctl status docker

docker --version

#Docker version 17.03.1-ce, build c6d412e

################################################################
## 사전작업 - kubelet kubeadm kubectl 설치
################################################################

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

setenforce 0

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

systemctl status kubelet



###cgroup 드라이버를 도커와 kube간에 일치 시켜줘야함
#v1.11버전에서는 자동으로 맞춰주므로 PASS
docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

#만약 systemd가 아니고 cgroupfs인 경우
#sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
#systemctl daemon-reload
#systemctl restart kubelet
#systemctl status kubelet


################################################################
## Setting up the cluster
################################################################

###### 1.Configure the kubelet to be a service manager for etcd.
#etcd 3대 모두 실행

systemctl stop kubelet


cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet


##### 2.Create configuration files for kubeadm.
#1번 호스트만 실행 

# Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts
export HOST0=10.10.64.96
export HOST1=10.10.64.97
export HOST2=10.10.64.98



# Create temp directories to store files that will end up on other hosts.
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/
ll /tmp


ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1alpha2"
kind: MasterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
EOF
done


cat /tmp/10.10.64.96/kubeadmcfg.yaml

cat /tmp/10.10.64.97/kubeadmcfg.yaml

cat /tmp/10.10.64.98/kubeadmcfg.yaml

##### 3.Generate the certificate authority
#1번 호스트에서 실행

kubeadm alpha phase certs etcd-ca

#그럼 아래 디렉토리와 파일 2개가 생성
cd /etc/kubernetes/pki/etcd
ll

##### 4.Create certificates for each member
#1번 호스트에서 실행(계속)

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# cleanup non-reusable certificates
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm alpha phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# No need to move the certs because they are for HOST0

# clean up certs that should not be copied off this host
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete


##### 5.Copy certificates and kubeadm configs

#1번 호스트에서 2번 호스트로 파일 복사

USER=root
HOST=${HOST1}

scp -r /tmp/${HOST}/* ${USER}@${HOST}:

ssh ${USER}@${HOST}
   sudo -Es
   chown -R root:root pki
   mv pki /etc/kubernetes/

#exit to host0
exit
exit


#1번 호스트에서 3번 호스트로 파일 복사

USER=root
HOST=${HOST2}

scp -r /tmp/${HOST}/* ${USER}@${HOST}:

ssh ${USER}@${HOST}
   sudo -Es
   chown -R root:root pki
   mv pki /etc/kubernetes/

#exit to host0
exit
exit



##### 6.Ensure all expected files exist
PASS

##### 7.Create the static pod manifests
#이제 각 서버에서 실행

#root@HOST0
systemctl stop kubelet
kubeadm alpha phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
systemctl start kubelet
systemctl status kubelet


#root@HOST1
systemctl stop kubelet
kubeadm alpha phase etcd local --config=/root/kubeadmcfg.yaml
systemctl start kubelet
systemctl status kubelet


#root@HOST2
systemctl stop kubelet
kubeadm alpha phase etcd local --config=/root/kubeadmcfg.yaml
systemctl start kubelet
systemctl status kubelet



#### 1번 호스트 실행하여 health check
docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \
--cert-file /etc/kubernetes/pki/etcd/peer.crt \
--key-file /etc/kubernetes/pki/etcd/peer.key \
--ca-file /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://${HOST0}:2379 cluster-health


 
